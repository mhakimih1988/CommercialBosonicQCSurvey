{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n",
        "# It can be customized to whatever you like\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Optimization & machine learning with TensorFlow\n",
        "===============================================\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The content in this page is suited to more advanced users who already have an understanding\n",
        "        of Strawberry Fields, e.g., those who have completed the `teleportation tutorial <tutorial>`.\n",
        "        Some basic knowledge of `TensorFlow <https://www.tensorflow.org/>`_ is also helpful.</p></div>\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial requires TensorFlow 2.0 and above. TensorFlow can be installed via ``pip``:\n",
        "\n",
        "    .. code-block:: console\n",
        "\n",
        "        pip install tensorflow\n",
        "\n",
        "    For more installation details and instructions, please refer to the\n",
        "    `TensorFlow documentation <https://www.tensorflow.org/install>`_.</p></div>\n",
        "\n",
        "In this demonstration, we show how the user can carry out optimization and machine learning on quantum\n",
        "circuits in Strawberry Fields. This functionality is provided via the TensorFlow simulator\n",
        "backend. By leveraging TensorFlow, we have access to a number of additional functionalities,\n",
        "including GPU integration, automatic gradient computation, built-in optimization algorithms,\n",
        "and other machine learning tools.\n",
        "\n",
        "Basic functionality\n",
        "-------------------\n",
        "\n",
        "As usual, we can initialize a Strawberry Fields program:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import strawberryfields as sf\n",
        "from strawberryfields.ops import *\n",
        "\n",
        "prog = sf.Program(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replacing numbers with Tensors\n",
        "------------------------------\n",
        "\n",
        "When a circuit contains only numerical parameters, the TensorFlow backend works the same\n",
        "as the other backends. However, with the TensorFlow backend, we have the additional option\n",
        "to use TensorFlow ``tf.Variable`` and ``tf.tensor`` objects for the parameters of Blackbird\n",
        "states, gates, and measurements.\n",
        "\n",
        "To construct the circuit to allow for TensorFlow objects, we **must** use **symbolic\n",
        "parameters** as the gate arguments. These can be created via the ``Program.params()``\n",
        "method:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# we can create symbolic parameters one by one\n",
        "alpha = prog.params(\"alpha\")\n",
        "\n",
        "# or create multiple at the same time\n",
        "theta_bs, phi_bs = prog.params(\"theta_bs\", \"phi_bs\")\n",
        "\n",
        "with prog.context as q:\n",
        "    # States\n",
        "    Coherent(alpha) | q[0]\n",
        "    # Gates\n",
        "    BSgate(theta_bs, phi_bs) | (q[0], q[1])\n",
        "    # Measurements\n",
        "    MeasureHomodyne(0.0) | q[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run a Strawberry Fields simulation with the TensorFlow backend, we need to specify\n",
        "``'tf'`` as the backend argument when initializing the engine:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 7})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now run our program using :meth:`eng.run() <strawberryfields.Engine.run>`. However,\n",
        "directly evaluating a circuit which contains symbolic parameters using :meth:`eng.run()\n",
        "<strawberryfields.Engine.run>` will produce errors. The reason for this is that :meth:`eng.run()\n",
        "<strawberryfields.Engine.run>` tries, by default, to numerically evaluate any measurement result.\n",
        "But we have not provided numerical values for the symbolic arguments yet!\n",
        "\n",
        ".. code-block:: python3\n",
        "\n",
        "    eng.run(prog)\n",
        "\n",
        ".. rst-class:: sphx-glr-script-out\n",
        "\n",
        "    Out:\n",
        "\n",
        "    .. code-block:: none\n",
        "\n",
        "        ParameterError: {alpha}: unbound parameter with no default value.\n",
        "\n",
        "To bind a numerical value to the free parameters, we pass a mapping from parameter\n",
        "name to value using the ``args`` keyword argument when running the engine:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float64.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        }
      ],
      "source": [
        "mapping = {\"alpha\": tf.Variable(0.5), \"theta_bs\": tf.constant(0.4), \"phi_bs\": tf.constant(0.0)}\n",
        "\n",
        "result = eng.run(prog, args=mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code will execute without error, and both the output results and the\n",
        "output samples will contain numeric values based on the given value for the angle ``phi``:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.55490553+0.j]], shape=(1, 1), dtype=complex64)\n"
          ]
        }
      ],
      "source": [
        "print(result.samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can select measurement results at other angles by supplying different values for\n",
        "``phi`` in ``mapping``. We can also return and perform processing on the underlying state:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Density matrix element [0,0,1,2]: tf.Tensor((0.0050209262+0j), shape=(), dtype=complex64)\n"
          ]
        }
      ],
      "source": [
        "state = result.state\n",
        "print(\"Density matrix element [0,0,1,2]:\", state.dm()[0, 0, 1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing gradients\n",
        "-------------------\n",
        "\n",
        "When used within a gradient tape context, any output generated from the state\n",
        "class can be differentiated natively using ``GradientTape().gradient()``.\n",
        "For example, lets displace a one-mode program, and then compute the gradient\n",
        "of the mean photon number:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient: [<tf.Tensor: shape=(), dtype=float32, numpy=0.8599996566772461>]\n"
          ]
        }
      ],
      "source": [
        "eng.reset()\n",
        "prog = sf.Program(1)\n",
        "\n",
        "alpha = prog.params(\"alpha\")\n",
        "\n",
        "with prog.context as q:\n",
        "    Dgate(alpha) | q\n",
        "\n",
        "# Assign our TensorFlow variables, so that we can\n",
        "# refer to them later when differentiating/training.\n",
        "a = tf.Variable(0.43)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Here, we map our quantum free parameter `alpha`\n",
        "    # to our TensorFlow variable `a` and pass it to the engine.\n",
        "\n",
        "    result = eng.run(prog, args={\"alpha\": a})\n",
        "    state = result.state\n",
        "\n",
        "    # Note that all processing, including state-based post-processing,\n",
        "    # must be done within the gradient tape context!\n",
        "    mean, var = state.mean_photon(0)\n",
        "\n",
        "# test that the gradient of the mean photon number is correct\n",
        "\n",
        "grad = tape.gradient(mean, [a])\n",
        "print(\"Gradient:\", grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean photon number of a displaced state $|\\alpha\\rangle$ is given by\n",
        "$\\langle \\hat{n} \\rangle(\\alpha) = |\\alpha|^2$, and thus, for real-valued\n",
        "$\\alpha$, the gradient is given by $\\langle \\hat{n}\\rangle'(\\alpha) = 2\\alpha$:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact gradient: tf.Tensor(0.86, shape=(), dtype=float32)\n",
            "Exact and TensorFlow gradient agree: True\n"
          ]
        }
      ],
      "source": [
        "print(\"Exact gradient:\", 2 * a)\n",
        "print(\"Exact and TensorFlow gradient agree:\", np.allclose(grad, 2 * a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Processing data\n",
        "---------------\n",
        "\n",
        "The parameters for Blackbird states, gates, and measurements may be more complex\n",
        "than just raw data or machine learning weights. These can themselves be the outputs\n",
        "from some learnable function like a neural network [#]_.\n",
        "\n",
        "We can also use the ``sf.math`` module to allow arbitrary processing of measurement\n",
        "results within the TensorFlow backend, by manipulating the ``.par`` attribute of\n",
        "the measured register. Note that the math functions in ``sf.math`` will be automatically\n",
        "converted to the equivalent TensorFlow ``tf.math`` function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float64.  This will discard the imaginary part and may not be what you intended.\n",
            "Measured Homodyne sample from mode 0: tf.Tensor((1.391514+0j), shape=(), dtype=complex64)\n",
            "Mean photon number of mode 0: tf.Tensor(0.0, shape=(), dtype=float32)\n",
            "Mean photon number of mode 1: tf.Tensor(0.967714, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "eng.reset()\n",
        "prog = sf.Program(2)\n",
        "\n",
        "with prog.context as q:\n",
        "    MeasureX | q[0]\n",
        "    Dgate(sf.math.sin(q[0].par)) | q[1]\n",
        "\n",
        "result = eng.run(prog)\n",
        "print(\"Measured Homodyne sample from mode 0:\", result.samples[0][0])\n",
        "\n",
        "mean, var = result.state.mean_photon(0)\n",
        "print(\"Mean photon number of mode 0:\", mean)\n",
        "\n",
        "mean, var = result.state.mean_photon(1)\n",
        "print(\"Mean photon number of mode 1:\", mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean photon number of mode 1 should be given by $\\sin(q[0])^2$, where $q[0]$\n",
        "is the measured Homodyne sample value:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.9682007+0j)\n"
          ]
        }
      ],
      "source": [
        "q0 = result.samples[0][0]\n",
        "print(np.sin(q0) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Working with batches\n",
        "--------------------\n",
        "\n",
        "It is common in machine learning to process data in *batches*. Strawberry Fields supports both\n",
        "unbatched and batched data when using the TensorFlow backend. Unbatched operation is the default\n",
        "behaviour (shown above). To enable batched operation, you should provide an extra\n",
        "``batch_size`` argument within the ``backend_options`` dictionary [#]_, e.g.,\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean photon number of mode 0 (batched): tf.Tensor([0.09999999 0.19999997 0.30000007], shape=(3,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# run simulation in batched-processing mode\n",
        "batch_size = 3\n",
        "prog = sf.Program(1)\n",
        "eng = sf.Engine(\"tf\", backend_options={\"cutoff_dim\": 15, \"batch_size\": batch_size})\n",
        "\n",
        "x = prog.params(\"x\")\n",
        "\n",
        "with prog.context as q:\n",
        "    Thermal(x) | q[0]\n",
        "\n",
        "x_val = tf.Variable([0.1, 0.2, 0.3])\n",
        "result = eng.run(prog, args={\"x\": x_val})\n",
        "print(\"Mean photon number of mode 0 (batched):\", result.state.mean_photon(0)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The batch size should be static, i.e., not changing over the course of a computation.</p></div>\n",
        "\n",
        "Parameters supplied to a circuit in batch-mode operation can either be scalars or vectors\n",
        "(of length ``batch_size``). Scalars are automatically broadcast over the batch dimension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha = tf.Variable([0.5] * batch_size)\n",
        "theta = tf.constant(0.0)\n",
        "phi = tf.Variable([0.1, 0.33, 0.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Measurement results will be returned as Tensors with shape ``(batch_size,)``.\n",
        "We can picture batch-mode operation as simulating multiple circuit configurations at the\n",
        "same time. Combined with appropriate parallelized hardware like GPUs, this can result in\n",
        "significant speedups compared to serial evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example: variational quantum circuit optimization\n",
        "-------------------------------------------------\n",
        "\n",
        "A key element of machine learning is optimization. We can use TensorFlow's automatic differentiation\n",
        "tools to optimize the parameters of *variational quantum circuits*. In this approach, we fix a\n",
        "circuit architecture where the states, gates, and/or measurements may have learnable parameters\n",
        "associated with them. We then define a loss function based on the output state of this circuit.\n",
        "\n",
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The state representation in the simulator can change from a ket (pure) to a density\n",
        "    matrix (mixed) if we use certain operations (e.g., state preparations). We can check ``state.is_pure``\n",
        "    to determine which representation is being used.</p></div>\n",
        "\n",
        "In the example below, we optimize a :class:`~strawberryfields.ops.Dgate` to produce an output with the largest overlap\n",
        "with the Fock state $n=1$. In this example, we compute the loss function imperatively\n",
        "within a gradient tape, and then apply the gradients using the chosen optimizer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability at step 0: 0.009900499135255814\n",
            "Probability at step 1: 0.03843073546886444\n",
            "Probability at step 2: 0.08083318173885345\n",
            "Probability at step 3: 0.13312943279743195\n",
            "Probability at step 4: 0.19034872949123383\n",
            "Probability at step 5: 0.24677586555480957\n",
            "Probability at step 6: 0.29659417271614075\n",
            "Probability at step 7: 0.3348402976989746\n",
            "Probability at step 8: 0.358536034822464\n",
            "Probability at step 9: 0.36760634183883667\n",
            "Probability at step 10: 0.3649459481239319\n",
            "Probability at step 11: 0.355257511138916\n",
            "Probability at step 12: 0.3432552218437195\n",
            "Probability at step 13: 0.332387775182724\n",
            "Probability at step 14: 0.3245660960674286\n",
            "Probability at step 15: 0.3205091059207916\n",
            "Probability at step 16: 0.3201930522918701\n",
            "Probability at step 17: 0.32317352294921875\n",
            "Probability at step 18: 0.3287633955478668\n",
            "Probability at step 19: 0.33612069487571716\n",
            "Probability at step 20: 0.3443042039871216\n",
            "Probability at step 21: 0.3523348271846771\n",
            "Probability at step 22: 0.3592871427536011\n",
            "Probability at step 23: 0.36441150307655334\n",
            "Probability at step 24: 0.3672657012939453\n",
            "Probability at step 25: 0.3678153157234192\n",
            "Probability at step 26: 0.3664506673812866\n",
            "Probability at step 27: 0.36389219760894775\n",
            "Probability at step 28: 0.3610053062438965\n",
            "Probability at step 29: 0.3585858643054962\n",
            "Probability at step 30: 0.3571901321411133\n",
            "Probability at step 31: 0.3570527136325836\n",
            "Probability at step 32: 0.35809433460235596\n",
            "Probability at step 33: 0.35999545454978943\n",
            "Probability at step 34: 0.3623036742210388\n",
            "Probability at step 35: 0.3645506203174591\n",
            "Probability at step 36: 0.36635252833366394\n",
            "Probability at step 37: 0.3674788475036621\n",
            "Probability at step 38: 0.3678767681121826\n",
            "Probability at step 39: 0.3676525950431824\n",
            "Probability at step 40: 0.3670198917388916\n",
            "Probability at step 41: 0.3662339746952057\n",
            "Probability at step 42: 0.3655301332473755\n",
            "Probability at step 43: 0.3650795519351959\n",
            "Probability at step 44: 0.3649671971797943\n",
            "Probability at step 45: 0.365190327167511\n",
            "Probability at step 46: 0.36567458510398865\n",
            "Probability at step 47: 0.3662993609905243\n",
            "Probability at step 48: 0.3669297695159912\n",
            "Probability at step 49: 0.3674468994140625\n"
          ]
        }
      ],
      "source": [
        "# initialize engine and program objects\n",
        "eng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 7})\n",
        "circuit = sf.Program(1)\n",
        "\n",
        "tf_alpha = tf.Variable(0.1)\n",
        "tf_phi = tf.Variable(0.1)\n",
        "\n",
        "alpha, phi = circuit.params(\"alpha\", \"phi\")\n",
        "\n",
        "with circuit.context as q:\n",
        "    Dgate(alpha, phi) | q[0]\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "steps = 50\n",
        "\n",
        "for step in range(steps):\n",
        "\n",
        "    # reset the engine if it has already been executed\n",
        "    if eng.run_progs:\n",
        "        eng.reset()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # execute the engine\n",
        "        results = eng.run(circuit, args={\"alpha\": tf_alpha, \"phi\": tf_phi})\n",
        "        # get the probability of fock state |1>\n",
        "        prob = results.state.fock_prob([1])\n",
        "        # negative sign to maximize prob\n",
        "        loss = -prob\n",
        "\n",
        "    gradients = tape.gradient(loss, [tf_alpha, tf_phi])\n",
        "    opt.apply_gradients(zip(gradients, [tf_alpha, tf_phi]))\n",
        "    print(\"Probability at step {}: {}\".format(step, prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After 50 or so iterations, the optimization should converge to the optimal probability value of\n",
        "$e^{-1}\\approx 0.3678$ at a parameter of $\\alpha=1$ [#]_.\n",
        "\n",
        "In cases where we want to take advantage of TensorFlow's graph mode,\n",
        "or do not need to extract/accumulate intermediate values used to calculate\n",
        "the cost function, we could also make use of TensorFlow's ``Optimizer().minimize``\n",
        "method, which requires the loss be defined as a function with no arguments:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Adam' object has no attribute 'minimize'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m tf_phi \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# In eager mode, calling Optimizer.minimize performs a single optimization step,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# and automatically updates the parameter values.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m(loss, [tf_alpha, tf_phi])\n\u001b[0;32m     22\u001b[0m     parameter_vals \u001b[38;5;241m=\u001b[39m [tf_alpha\u001b[38;5;241m.\u001b[39mnumpy(), tf_phi\u001b[38;5;241m.\u001b[39mnumpy()]\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter values at step \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(step, parameter_vals))\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'minimize'"
          ]
        }
      ],
      "source": [
        "def loss():\n",
        "    # reset the engine if it has already been executed\n",
        "    if eng.run_progs:\n",
        "        eng.reset()\n",
        "\n",
        "    # execute the engine\n",
        "    results = eng.run(circuit, args={\"alpha\": tf_alpha, \"phi\": tf_phi})\n",
        "    # get the probability of fock state |1>\n",
        "    prob = results.state.fock_prob([1])\n",
        "    # negative sign to maximize prob\n",
        "    return -prob\n",
        "\n",
        "\n",
        "tf_alpha = tf.Variable(0.1)\n",
        "tf_phi = tf.Variable(0.1)\n",
        "\n",
        "\n",
        "for step in range(steps):\n",
        "    # In eager mode, calling Optimizer.minimize performs a single optimization step,\n",
        "    # and automatically updates the parameter values.\n",
        "    _ = opt.minimize(loss, [tf_alpha, tf_phi])\n",
        "    parameter_vals = [tf_alpha.numpy(), tf_phi.numpy()]\n",
        "    print(\"Parameter values at step {}: {}\".format(step, parameter_vals))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other high-level optimization interfaces, such as Keras, may also\n",
        "be used to train models built using the Strawberry Fields TensorFlow backend.\n",
        "\n",
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>When optimizing circuits which contains energy-changing operations (displacement, squeezing, etc.),\n",
        "    one should be careful to monitor that the state does not leak out of the given truncation level. This can\n",
        "    be accomplished by regularizing or using ``tf.clip_by_value`` on the relevant parameters.</p></div>\n",
        "\n",
        "TensorFlow supports a large set of mathematical and machine learning operations which can be applied to\n",
        "a circuit's output state to enable further processing. Examples include ``tf.norm``,\n",
        "``tf.linalg.eigh``, ``tf.linalg.svd``, and ``tf.linalg.inv`` [#]_.\n",
        "\n",
        "Exercise: Hong-Ou-Mandel Effect\n",
        "-------------------------------\n",
        "\n",
        "Use the optimization methods outlined above to find the famous\n",
        "`Hong-Ou-Mandel effect <https://en.wikipedia.org/wiki/Hong%E2%80%93Ou%E2%80%93Mandel_effect>`_,\n",
        "where photons bunch together in the same mode. Your circuit should contain two modes, each with a\n",
        "single-photon input state, and a beamsplitter with variable parameters. By optimizing the beamsplitter\n",
        "parameters, minimize the probability of the $|1,1\\rangle$ Fock-basis element of the output state.\n",
        "\n",
        ".. rubric:: Footnotes\n",
        "\n",
        ".. [#] Note that certain operations---in particular, measurements---may not have gradients defined within\n",
        "       TensorFlow. When optimizing via gradient descent, we must be careful to define a circuit which is end-to-end differentiable.\n",
        "\n",
        "\n",
        ".. [#] Note that ``batch_size`` should not be set to 1. Instead, use ``batch_size=None``, or just\n",
        "       omit the ``batch_size`` argument.\n",
        "\n",
        ".. [#] In this tutorial, we have applied classical machine learning tools to learn a quantum optical circuit.\n",
        "       Of course, there are many other possibilities for combining machine learning and quantum computing,\n",
        "       e.g., using quantum algorithms to speed up machine learning subroutines, or fully quantum learning on\n",
        "       unprocessed quantum data.\n",
        "\n",
        ".. [#] Remember that it might be necessary to reshape a ket or density matrix before using some of these functions.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "StrawberryFieldsVenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
